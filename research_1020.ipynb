{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/JiHoonPark96/practice/blob/main/research_1020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVyna4tecFoz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyTDC\n",
      "  Using cached pytdc-1.1.1.tar.gz (146 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting accelerate==0.33.0 (from PyTDC)\n",
      "  Using cached accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting biopython<2.0,>=1.78 (from PyTDC)\n",
      "  Using cached biopython-1.84.tar.gz (25.8 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting dataclasses<1.0,>=0.6 (from PyTDC)\n",
      "  Using cached dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting datasets==2.20.0 (from PyTDC)\n",
      "  Using cached datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate==0.4.2 (from PyTDC)\n",
      "  Using cached evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting fuzzywuzzy<1.0,>=0.18.0 (from PyTDC)\n",
      "  Using cached fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.20.3 (from PyTDC)\n",
      "  Using cached huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting moleculeace==3.0.0 (from PyTDC)\n",
      "  Using cached MoleculeACE-3.0.0-py3-none-any.whl.metadata (549 bytes)\n",
      "Collecting mygene<4.0.0,>=3.2.2 (from PyTDC)\n",
      "  Using cached mygene-3.2.2-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting numpy<2.0.0,>=1.26.4 (from PyTDC)\n",
      "  Using cached numpy-1.26.4.tar.gz (15.8 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [21 lines of output]\n",
      "      + c:\\Users\\starw\\AppData\\Local\\Programs\\Python\\Python313\\python.exe C:\\Users\\starw\\AppData\\Local\\Temp\\pip-install-vt4sbhod\\numpy_e5e2b1ed6d57402c822e3d43bb2ef193\\vendored-meson\\meson\\meson.py setup C:\\Users\\starw\\AppData\\Local\\Temp\\pip-install-vt4sbhod\\numpy_e5e2b1ed6d57402c822e3d43bb2ef193 C:\\Users\\starw\\AppData\\Local\\Temp\\pip-install-vt4sbhod\\numpy_e5e2b1ed6d57402c822e3d43bb2ef193\\.mesonpy-_3wkle9t -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\starw\\AppData\\Local\\Temp\\pip-install-vt4sbhod\\numpy_e5e2b1ed6d57402c822e3d43bb2ef193\\.mesonpy-_3wkle9t\\meson-python-native-file.ini\n",
      "      The Meson build system\n",
      "      Version: 1.2.99\n",
      "      Source dir: C:\\Users\\starw\\AppData\\Local\\Temp\\pip-install-vt4sbhod\\numpy_e5e2b1ed6d57402c822e3d43bb2ef193\n",
      "      Build dir: C:\\Users\\starw\\AppData\\Local\\Temp\\pip-install-vt4sbhod\\numpy_e5e2b1ed6d57402c822e3d43bb2ef193\\.mesonpy-_3wkle9t\n",
      "      Build type: native build\n",
      "      Project name: NumPy\n",
      "      Project version: 1.26.4\n",
      "      WARNING: Failed to activate VS environment: Could not find C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\n",
      "      \n",
      "      ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
      "      The following exception(s) were encountered:\n",
      "      Running `icl \"\"` gave \"[WinError 2] ì§€ì •ë\\x90œ íŒŒì\\x9d¼ì\\x9d„ ì°¾ì\\x9d„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\"\n",
      "      Running `cl /?` gave \"[WinError 2] ì§€ì •ë\\x90œ íŒŒì\\x9d¼ì\\x9d„ ì°¾ì\\x9d„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\"\n",
      "      Running `cc --version` gave \"[WinError 2] ì§€ì •ë\\x90œ íŒŒì\\x9d¼ì\\x9d„ ì°¾ì\\x9d„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\"\n",
      "      Running `gcc --version` gave \"[WinError 2] ì§€ì •ë\\x90œ íŒŒì\\x9d¼ì\\x9d„ ì°¾ì\\x9d„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\"\n",
      "      Running `clang --version` gave \"[WinError 2] ì§€ì •ë\\x90œ íŒŒì\\x9d¼ì\\x9d„ ì°¾ì\\x9d„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\"\n",
      "      Running `clang-cl /?` gave \"[WinError 2] ì§€ì •ë\\x90œ íŒŒì\\x9d¼ì\\x9d„ ì°¾ì\\x9d„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\"\n",
      "      Running `pgcc --version` gave \"[WinError 2] ì§€ì •ë\\x90œ íŒŒì\\x9d¼ì\\x9d„ ì°¾ì\\x9d„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\"\n",
      "      \n",
      "      A full log can be found at C:\\Users\\starw\\AppData\\Local\\Temp\\pip-install-vt4sbhod\\numpy_e5e2b1ed6d57402c822e3d43bb2ef193\\.mesonpy-_3wkle9t\\meson-logs\\meson-log.txt\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tdc\n",
      "  Using cached tdc-0.1-py3-none-any.whl.metadata (635 bytes)\n",
      "Collecting wikipedia (from tdc)\n",
      "  Using cached wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting requests (from tdc)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting boto3 (from tdc)\n",
      "  Using cached boto3-1.35.53-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting python-vlc (from tdc)\n",
      "  Using cached python_vlc-3.0.21203-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting chatterbot (from tdc)\n",
      "  Using cached ChatterBot-1.0.5-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting pyserial (from tdc)\n",
      "  Using cached pyserial-3.5-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting speechrecognition (from tdc)\n",
      "  Using cached SpeechRecognition-3.11.0-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Collecting google-speech (from tdc)\n",
      "  Using cached google_speech-1.2.0.tar.gz (17 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting botocore<1.36.0,>=1.35.53 (from boto3->tdc)\n",
      "  Using cached botocore-1.35.53-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->tdc)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->tdc)\n",
      "  Using cached s3transfer-0.10.3-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting mathparse<0.2,>=0.1 (from chatterbot->tdc)\n",
      "  Using cached mathparse-0.1.2-py3-none-any.whl.metadata (776 bytes)\n",
      "Collecting nltk<4.0,>=3.2 (from chatterbot->tdc)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pint>=0.8.1 (from chatterbot->tdc)\n",
      "  Using cached Pint-0.24.3-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting pymongo<4.0,>=3.3 (from chatterbot->tdc)\n",
      "  Using cached pymongo-3.13.0.tar.gz (804 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting python-dateutil<2.8,>=2.7 (from chatterbot->tdc)\n",
      "  Using cached python_dateutil-2.7.5-py2.py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting pyyaml<5.2,>=5.1 (from chatterbot->tdc)\n",
      "  Using cached PyYAML-5.1.2.tar.gz (265 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting spacy<2.2,>=2.1 (from chatterbot->tdc)\n",
      "  Using cached spacy-2.1.9.tar.gz (30.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × pip subprocess to install build dependencies did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [58 lines of output]\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-75.3.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "      Collecting wheel<0.33.0,>0.32.0\n",
      "        Using cached wheel-0.32.3-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "      Collecting Cython\n",
      "        Using cached Cython-3.0.11-cp313-cp313-win_amd64.whl.metadata (3.2 kB)\n",
      "      Collecting cymem<2.1.0,>=2.0.2\n",
      "        Using cached cymem-2.0.8.tar.gz (9.8 kB)\n",
      "        Installing build dependencies: started\n",
      "        Installing build dependencies: finished with status 'done'\n",
      "        Getting requirements to build wheel: started\n",
      "        Getting requirements to build wheel: finished with status 'done'\n",
      "        Preparing metadata (pyproject.toml): started\n",
      "        Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "      Collecting preshed<2.1.0,>=2.0.1\n",
      "        Using cached preshed-2.0.1.tar.gz (113 kB)\n",
      "        Installing build dependencies: started\n",
      "        Installing build dependencies: finished with status 'done'\n",
      "        Getting requirements to build wheel: started\n",
      "        Getting requirements to build wheel: finished with status 'error'\n",
      "        error: subprocess-exited-with-error\n",
      "      \n",
      "        Ã— Getting requirements to build wheel did not run successfully.\n",
      "        â”‚ exit code: 1\n",
      "        â•°â”€> [23 lines of output]\n",
      "            Traceback (most recent call last):\n",
      "              File \"C:\\Users\\starw\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "                main()\n",
      "                ~~~~^^\n",
      "              File \"C:\\Users\\starw\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "                json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                                         ~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "              File \"C:\\Users\\starw\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "                return hook(config_settings)\n",
      "              File \"C:\\Users\\starw\\AppData\\Local\\Temp\\pip-build-env-5xx371l_\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 333, in get_requires_for_build_wheel\n",
      "                return self._get_build_requires(config_settings, requirements=[])\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "              File \"C:\\Users\\starw\\AppData\\Local\\Temp\\pip-build-env-5xx371l_\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 303, in _get_build_requires\n",
      "                self.run_setup()\n",
      "                ~~~~~~~~~~~~~~^^\n",
      "              File \"C:\\Users\\starw\\AppData\\Local\\Temp\\pip-build-env-5xx371l_\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 521, in run_setup\n",
      "                super().run_setup(setup_script=setup_script)\n",
      "                ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "              File \"C:\\Users\\starw\\AppData\\Local\\Temp\\pip-build-env-5xx371l_\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 319, in run_setup\n",
      "                exec(code, locals())\n",
      "                ~~~~^^^^^^^^^^^^^^^^\n",
      "              File \"<string>\", line 9, in <module>\n",
      "            ImportError: cannot import name 'msvccompiler' from 'distutils' (C:\\Users\\starw\\AppData\\Local\\Temp\\pip-build-env-5xx371l_\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\__init__.py). Did you mean: 'ccompiler'?\n",
      "            [end of output]\n",
      "      \n",
      "        note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "      error: subprocess-exited-with-error\n",
      "      \n",
      "      Ã— Getting requirements to build wheel did not run successfully.\n",
      "      â”‚ exit code: 1\n",
      "      â•°â”€> See above for output.\n",
      "      \n",
      "      note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× pip subprocess to install build dependencies did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tdc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall tdc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# TDC 라이브러리에서 PPI 데이터셋 불러오기\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtdc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_pred\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PPI\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Import necessary libraries\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tdc'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Install necessary packages\n",
    "%pip install PyTDC\n",
    "%pip install tdc\n",
    "# TDC 라이브러리에서 PPI 데이터셋 불러오기\n",
    "from tdc.multi_pred import PPI\n",
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# HuRI 데이터셋 불러오기\n",
    "import tdc.multi_pred\n",
    "data = tdc.multi_pred.PPI(name='HuRI')\n",
    "data = PPI(name='HuRI')\n",
    "\n",
    "# 데이터셋을 훈련용(train), 검증용(validation), 테스트용(test)으로 분할\n",
    "split = data.get_split()\n",
    "# Load the HuRI dataset\n",
    "try:\n",
    "    data = PPI(name='HuRI')\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to load dataset: {e}\")\n",
    "\n",
    "# 각 분할 데이터셋을 Pandas DataFrame으로 변환\n",
    "train_data = split['train']\n",
    "# Split the dataset into training, validation, and test sets\n",
    "try:\n",
    "    split = data.get_split()\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to split dataset: {e}\")\n",
    "valid_data = split['valid']\n",
    "test_data = split['test']\n",
    "\n",
    "# Convert each split into a Pandas DataFrame\n",
    "try:\n",
    "    train_data = split.get('train')\n",
    "    valid_data = split.get('valid')\n",
    "    test_data = split.get('test')\n",
    "    if train_data is None or valid_data is None or test_data is None:\n",
    "        raise ValueError(\"One or more dataset splits are missing\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error processing dataset splits: {e}\")\n",
    "# 데이터 확인\n",
    "print(train_data.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGafTn5ZcG4n"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 1. 단백질 서열을 숫자로 변환하기 위한 Tokenizer 생성\n",
    "tokenizer = Tokenizer(char_level=True)  # 아미노산 각각을 문자 단위로 처리\n",
    "tokenizer.fit_on_texts(train_data['Protein1'].tolist() + train_data['Protein2'].tolist())\n",
    "\n",
    "# 2. 각 단백질 서열을 숫자 시퀀스로 변환\n",
    "X1_train = tokenizer.texts_to_sequences(train_data['Protein1'].tolist())\n",
    "X2_train = tokenizer.texts_to_sequences(train_data['Protein2'].tolist())\n",
    "\n",
    "X1_valid = tokenizer.texts_to_sequences(valid_data['Protein1'].tolist())\n",
    "X2_valid = tokenizer.texts_to_sequences(valid_data['Protein2'].tolist())\n",
    "\n",
    "X1_test = tokenizer.texts_to_sequences(test_data['Protein1'].tolist())\n",
    "X2_test = tokenizer.texts_to_sequences(test_data['Protein2'].tolist())\n",
    "\n",
    "# 3. 시퀀스 패딩: 서열의 길이가 다를 수 있으므로 고정된 길이로 맞추기 위해 패딩\n",
    "max_length = 100  # 최대 길이를 100으로 설정 (서열에 따라 적절히 변경 가능)\n",
    "X1_train = pad_sequences(X1_train, maxlen=max_length, padding='post')\n",
    "X2_train = pad_sequences(X2_train, maxlen=max_length, padding='post')\n",
    "\n",
    "X1_valid = pad_sequences(X1_valid, maxlen=max_length, padding='post')\n",
    "X2_valid = pad_sequences(X2_valid, maxlen=max_length, padding='post')\n",
    "\n",
    "X1_test = pad_sequences(X1_test, maxlen=max_length, padding='post')\n",
    "X2_test = pad_sequences(X2_test, maxlen=max_length, padding='post')\n",
    "\n",
    "# 두 단백질 서열을 하나로 결합하여 모델 입력 데이터로 사용\n",
    "import numpy as np\n",
    "X_train = np.concatenate((X1_train, X2_train), axis=1)\n",
    "X_valid = np.concatenate((X1_valid, X2_valid), axis=1)\n",
    "X_test = np.concatenate((X1_test, X2_test), axis=1)\n",
    "\n",
    "# 타겟 값\n",
    "y_train = train_data['Y']\n",
    "y_valid = valid_data['Y']\n",
    "y_test = test_data['Y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ziUxRIFcJGu"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
    "\n",
    "# 1. 임베딩 레이어를 사용하는 신경망 모델 구성\n",
    "vocab_size = len(tokenizer.word_index) + 1  # 아미노산 종류 수 (Tokenizer에서 학습된 아미노산 개수)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=2*max_length))  # 두 단백질 서열을 합친 길이\n",
    "model.add(Flatten())  # 1차원으로 변환\n",
    "model.add(Dense(128, activation='relu'))  # 은닉층\n",
    "model.add(Dense(64, activation='relu'))  # 은닉층\n",
    "model.add(Dense(1, activation='sigmoid'))  # 출력층 (이진 분류)\n",
    "\n",
    "# 2. 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 3. 모델 학습\n",
    "history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-XRjItDcLRG"
   },
   "outputs": [],
   "source": [
    "# 테스트 데이터로 성능 평가\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Target variable distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='Y', data=train_data)\n",
    "plt.title('Distribution of Target Variable (Y)')\n",
    "plt.show()\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = train_data.isnull().sum()\n",
    "print(\"Missing values in training data:\\n\", missing_values)\n",
    "\n",
    "# Distribution of protein sequence lengths\n",
    "train_data['Protein1_length'] = train_data['Protein1'].apply(len)\n",
    "train_data['Protein2_length'] = train_data['Protein2'].apply(len)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(train_data['Protein1_length'], bins=30, kde=True)\n",
    "plt.title('Distribution of Protein1 Sequence Lengths')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(train_data['Protein2_length'], bins=30, kde=True)\n",
    "plt.title('Distribution of Protein2 Sequence Lengths')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPYS+HSWSDjgHVf4RuhV9I8",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
