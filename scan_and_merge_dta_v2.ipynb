{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f3cd23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Arora_et_al_2021_Data ===\n",
      "\n",
      "=== Azoulay_et_al_2019_Data ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\starw\\AppData\\Local\\Temp\\ipykernel_22628\\3276869272.py:62: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  return pd.read_stata(p, convert_categoricals=False)\n",
      "C:\\Users\\starw\\AppData\\Local\\Temp\\ipykernel_22628\\3276869272.py:62: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  return pd.read_stata(p, convert_categoricals=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Berkouwer_and_Dean_2022_Data ===\n",
      "\n",
      "=== Cao_and_Chen_2022_Data ===\n",
      "\n",
      "=== Carranza_et_al_2022_Data ===\n",
      "\n",
      "=== Chen_and_Roth_2023_Data ===\n",
      "[WARN] no variables found in C:\\Users\\starw\\OneDrive\\바탕 화면\\dta\\Chen_and_Roth_2023_Data\n",
      "\n",
      "=== Fetzer_et_al_2021_Data ===\n",
      "[WARN] read fail: C:\\Users\\starw\\OneDrive\\바탕 화면\\dta\\Fetzer_et_al_2021_Data\\REPOSITORY\\PROCESSING_FILES\\DATA_INPUTS\\DEPLOYMENT\\ISAFSTRENGTH.dta | Version of given Stata file is 110. pandas supports importing versions 105, 108, 111 (Stata 7SE), 113 (Stata 8/9), 114 (Stata 10/11), 115 (Stata 12), 117 (Stata 13), 118 (Stata 14/15/16),and 119 (Stata 15/16, over 32,767 variables).\n",
      "[WARN] read fail: C:\\Users\\starw\\OneDrive\\바탕 화면\\dta\\Fetzer_et_al_2021_Data\\REPOSITORY\\PROCESSING_FILES\\DATA_INPUTS\\DISTANCE_MEASURES\\NEARROADS.dta | Version of given Stata file is 110. pandas supports importing versions 105, 108, 111 (Stata 7SE), 113 (Stata 8/9), 114 (Stata 10/11), 115 (Stata 12), 117 (Stata 13), 118 (Stata 14/15/16),and 119 (Stata 15/16, over 32,767 variables).\n",
      "[WARN] read fail: C:\\Users\\starw\\OneDrive\\바탕 화면\\dta\\Fetzer_et_al_2021_Data\\REPOSITORY\\PROCESSING_FILES\\DATA_INPUTS\\DISTANCE_MEASURES\\TRAVELDISTNEARMILAIRPORT.dta | Version of given Stata file is 110. pandas supports importing versions 105, 108, 111 (Stata 7SE), 113 (Stata 8/9), 114 (Stata 10/11), 115 (Stata 12), 117 (Stata 13), 118 (Stata 14/15/16),and 119 (Stata 15/16, over 32,767 variables).\n",
      "\n",
      "=== Hjort_and_Poulsen_2019_Data ===\n",
      "\n",
      "=== Johnson_2020_Data ===\n",
      "\n",
      "=== Moretti_2021_Data ===\n",
      "\n",
      "=== Rogall_2021_Data ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\starw\\AppData\\Local\\Temp\\ipykernel_22628\\3276869272.py:62: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  return pd.read_stata(p, convert_categoricals=False)\n",
      "C:\\Users\\starw\\AppData\\Local\\Temp\\ipykernel_22628\\3276869272.py:62: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  return pd.read_stata(p, convert_categoricals=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DONE] See: C:\\Users\\starw\\OneDrive\\바탕 화면\\dta_output\n"
     ]
    }
   ],
   "source": [
    "# scan_and_merge_dta_v2.py\n",
    "import os, re, warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "import pandas as pd\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "# =========================\n",
    "# 0) 경로/설정 (수정 가능)\n",
    "# =========================\n",
    "ROOT_DIR   = r\"C:\\Users\\starw\\OneDrive\\바탕 화면\\dta\"         # 11개 상위 폴더가 모여 있는 경로\n",
    "OUTPUT_DIR = r\"C:\\Users\\starw\\OneDrive\\바탕 화면\\dta_output\"  # 결과물이 저장될 경로\n",
    "GROUPS_TO_MERGE = [\"internet\",\"earnings\",\"hours\",\"conflict\",\"pubs\",\"rebellion\",\"violations\",\"cluster\"]\n",
    "\n",
    "# 키워드(원시 변수 후보)\n",
    "KEYWORD_GROUPS = {\n",
    "    \"internet\":   [\"internet\",\"fastinternet\",\"broadband\",\"speed\",\"kbps\",\"mbps\",\"users\",\"usage\",\"penetration\"],\n",
    "    \"earnings\":   [\"earnings\",\"income\",\"revenue\",\"wage\",\"wages\",\"hourly_wage\",\"pay\",\"salary\"],\n",
    "    \"hours\":      [\"hours\",\"hrs\",\"time_worked\"],\n",
    "    \"conflict\":   [\"incident\",\"incidents\",\"conflict\",\"sigacts\",\"ied\",\"direct_fire\",\"casualty\",\"killings\",\"victims\"],\n",
    "    \"pubs\":       [\"publication\",\"publications\",\"paper\",\"papers\",\"patent\",\"patents\",\"citation\",\"citations\"],\n",
    "    \"rebellion\":  [\"rebellion\",\"rebellions\",\"rebel\",\"uprising\"],\n",
    "    \"violations\": [\"violation\",\"violations\",\"penalty\",\"penalties\",\"fine\",\"fines\"],\n",
    "    \"cluster\":    [\"cluster_size\",\"inventors\",\"comets\",\"field_size\",\"peer_count\"],\n",
    "}\n",
    "# 변환 변수 패턴(원시 아님)\n",
    "TRANSFORM_PATTERNS = [r\"^log\", r\"^ln\", r\"^asinh\", r\"^ihs\"]\n",
    "\n",
    "# 병합 키 후보(상황 따라 수정)\n",
    "POSSIBLE_KEYS = [\n",
    "    [\"city\",\"country_year\"], [\"city\",\"year\"], [\"city_id\",\"year\"],\n",
    "    [\"country\",\"year\"], [\"geo_id\",\"year\"], [\"id\",\"year\"],\n",
    "    [\"region\",\"year\"], [\"state\",\"year\"]\n",
    "]\n",
    "\n",
    "# 분석용 파일 이름 힌트(없으면 가장 큰 dta를 선택)\n",
    "ANALYSIS_FILE_HINTS = [\"paper1\",\"analysis\",\"table\",\"final\",\"main\",\"reg\",\"ready\"]\n",
    "\n",
    "# 문서화용(실제 계산 X)\n",
    "SUGGESTED_TRANSFORMS = {\n",
    "    \"internet\":   \"loginternet = log(1 + internet) OR asinh(internet)\",\n",
    "    \"earnings\":   \"asinh(earnings) / asinh(wages)\",\n",
    "    \"hours\":      \"asinh(hours)\",\n",
    "    \"conflict\":   \"log(1 + incidents/killings)\",\n",
    "    \"pubs\":       \"log(publications) / log(citations)\",\n",
    "    \"rebellion\":  \"asinh(rebellions per capita)\",\n",
    "    \"violations\": \"log(violations) / log(penalties)\",\n",
    "    \"cluster\":    \"log(cluster size)\",\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# 1) 유틸 함수\n",
    "# =========================\n",
    "def rglob_dta(folder: Path) -> List[Path]:\n",
    "    return sorted(folder.rglob(\"*.dta\"))\n",
    "\n",
    "def looks_transformed(var_lower: str) -> bool:\n",
    "    return any(re.match(p, var_lower) for p in TRANSFORM_PATTERNS)\n",
    "\n",
    "def read_dta(p: Path) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        return pd.read_stata(p, convert_categoricals=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] read fail: {p} | {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_group(var_lower: str) -> Optional[str]:\n",
    "    for g, kws in KEYWORD_GROUPS.items():\n",
    "        for kw in kws:\n",
    "            if kw in var_lower:\n",
    "                return g\n",
    "    return None\n",
    "\n",
    "def lower_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    x = df.copy()\n",
    "    x.columns = [c.lower() for c in x.columns]\n",
    "    return x\n",
    "\n",
    "def pick_analysis_file(dtas: List[Path]) -> Optional[Path]:\n",
    "    if not dtas: return None\n",
    "    for hint in ANALYSIS_FILE_HINTS:\n",
    "        cands = [p for p in dtas if hint in p.stem.lower()]\n",
    "        if cands:\n",
    "            return max(cands, key=lambda p: p.stat().st_size)\n",
    "    return max(dtas, key=lambda p: p.stat().st_size)\n",
    "\n",
    "def pick_join_keys(paper: pd.DataFrame, src: pd.DataFrame) -> Optional[List[str]]:\n",
    "    pa = set(paper.columns); sb = set(src.columns)\n",
    "    for keys in POSSIBLE_KEYS:\n",
    "        k = [kk.lower() for kk in keys]\n",
    "        if set(k).issubset(pa) and set(k).issubset(sb):\n",
    "            return k\n",
    "    # fallback: 공통 컬럼 2개\n",
    "    cmn = list(pa & sb)\n",
    "    if len(cmn) >= 2: return cmn[:2]\n",
    "    return None\n",
    "\n",
    "def aggregate_duplicates(df: pd.DataFrame, keys: List[str]) -> pd.DataFrame:\n",
    "    g = df.groupby(keys, dropna=False)\n",
    "    if g.size().max() <= 1: return df\n",
    "    def agg(x):\n",
    "        return x.mean() if pd.api.types.is_numeric_dtype(x) else x.iloc[0]\n",
    "    return g.aggregate(agg).reset_index()\n",
    "\n",
    "def select_best_raw(df_vars: pd.DataFrame, group: str) -> Optional[tuple]:\n",
    "    cand = df_vars[(df_vars[\"group\"]==group) & (~df_vars[\"looks_transformed\"])].copy()\n",
    "    if cand.empty: return None\n",
    "    def score(r):\n",
    "        s = 0\n",
    "        v = r[\"variable_lower\"]; f = r[\"rel_path\"].lower()\n",
    "        if v == group: s += 3\n",
    "        if group in v: s += 2\n",
    "        if any(h in f for h in [\"panel\",\"city\",\"urban\",\"africa\",\"global\",\"main\",\"ready\"]): s += 1\n",
    "        s += max(0, 20 - len(v))\n",
    "        return s\n",
    "    cand[\"score\"] = cand.apply(score, axis=1)\n",
    "    cand = cand.sort_values([\"score\",\"rel_path\"], ascending=[False, True])\n",
    "    row = cand.iloc[0]\n",
    "    return (row[\"rel_path\"], row[\"variable\"])  # 상대경로, 변수명\n",
    "\n",
    "# =========================\n",
    "# 2) 폴더 단위 스캔 + 병합\n",
    "# =========================\n",
    "def scan_folder(root_sub: Path, out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    rows = []\n",
    "    dtas = rglob_dta(root_sub)\n",
    "    for dta in dtas:\n",
    "        df = read_dta(dta)\n",
    "        if df is None: continue\n",
    "        rel = str(dta.relative_to(root_sub))\n",
    "        for col in df.columns:\n",
    "            clo = col.lower()\n",
    "            rows.append({\n",
    "                \"folder\": root_sub.name,\n",
    "                \"rel_path\": rel,         # 하위폴더 포함 상대경로\n",
    "                \"file\": dta.name,\n",
    "                \"variable\": col,\n",
    "                \"variable_lower\": clo,\n",
    "                \"group\": detect_group(clo),\n",
    "                \"looks_transformed\": looks_transformed(clo),\n",
    "            })\n",
    "    var_df = pd.DataFrame(rows)\n",
    "    if var_df.empty:\n",
    "        print(f\"[WARN] no variables found in {root_sub}\")\n",
    "        return None, None, None\n",
    "\n",
    "    # 저장: 전체 변수 스캔\n",
    "    var_df.to_csv(out_dir / \"all_vars_scanned.csv\", index=False)\n",
    "\n",
    "    # 원시 후보만 요약 + 추천 변환식\n",
    "    candidates = var_df[(~var_df[\"looks_transformed\"]) & var_df[\"group\"].notna()].copy()\n",
    "    candidates[\"suggested_transform\"] = candidates[\"group\"].map(SUGGESTED_TRANSFORMS)\n",
    "    candidates.sort_values([\"rel_path\",\"group\",\"variable\"]).to_csv(out_dir / \"variable_map.csv\", index=False)\n",
    "\n",
    "    # 분석용 파일 선택\n",
    "    analysis = pick_analysis_file(dtas)\n",
    "    return var_df, candidates, analysis\n",
    "\n",
    "def merge_raw_into_analysis(root_sub: Path, analysis_path: Path, var_df: pd.DataFrame, out_dir: Path):\n",
    "    log_lines = []\n",
    "    if analysis_path is None:\n",
    "        (out_dir / \"merge_report.txt\").write_text(\"No analysis .dta detected.\", encoding=\"utf-8\")\n",
    "        return None\n",
    "\n",
    "    paper = lower_cols(read_dta(analysis_path))\n",
    "    merged = paper.copy()\n",
    "    log_lines.append(f\"Analysis file: {analysis_path}\")\n",
    "    log_lines.append(f\"Rows(original): {len(merged)}\")\n",
    "\n",
    "    for group in GROUPS_TO_MERGE:\n",
    "        sel = select_best_raw(var_df, group)\n",
    "        if sel is None:\n",
    "            log_lines.append(f\"[{group}] raw candidate not found.\")\n",
    "            continue\n",
    "\n",
    "        rel_src, varname = sel\n",
    "        src_path = root_sub / rel_src\n",
    "        src = read_dta(src_path)\n",
    "        if src is None:\n",
    "            log_lines.append(f\"[{group}] cannot read source: {rel_src}\")\n",
    "            continue\n",
    "\n",
    "        src_lo = lower_cols(src)\n",
    "        keys = pick_join_keys(merged, src_lo)\n",
    "        if not keys:\n",
    "            log_lines.append(f\"[{group}] join keys not found with {rel_src}. skipped.\")\n",
    "            continue\n",
    "\n",
    "        use_cols = list(dict.fromkeys(keys + [varname.lower()]))\n",
    "        # 실제 컬럼 이름 보정\n",
    "        use_cols = [c if c in src_lo.columns else c.lower() for c in use_cols]\n",
    "        use_cols = [c for c in use_cols if c in src_lo.columns]\n",
    "        src_narrow = src_lo[use_cols].copy()\n",
    "        src_narrow = aggregate_duplicates(src_narrow, keys)\n",
    "\n",
    "        before_cols = set(merged.columns)\n",
    "        merged = merged.merge(src_narrow, on=keys, how=\"left\", validate=\"m:1\")\n",
    "        newcol = varname.lower()\n",
    "        miss = merged[newcol].isna().sum() if newcol in merged.columns else -1\n",
    "        log_lines.append(f\"[{group}] merged from {rel_src} var={varname} | keys={keys} | missing_after_join={miss}\")\n",
    "\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_dta = out_dir / f\"{analysis_path.stem}_with_raw.dta\"\n",
    "    merged.to_stata(out_dta, write_index=False, version=118)\n",
    "    (out_dir / \"merge_report.txt\").write_text(\"\\n\".join(log_lines), encoding=\"utf-8\")\n",
    "    return out_dta\n",
    "\n",
    "# =========================\n",
    "# 3) 메인 루프\n",
    "# =========================\n",
    "def main():\n",
    "    root = Path(ROOT_DIR)\n",
    "    out_root = Path(OUTPUT_DIR)\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    master_rows = []\n",
    "\n",
    "    subfolders = [p for p in root.iterdir() if p.is_dir()]\n",
    "    for sub in sorted(subfolders, key=lambda p: p.name.lower()):\n",
    "        print(f\"\\n=== {sub.name} ===\")\n",
    "        out_dir = out_root / sub.name\n",
    "        var_df, cand_df, analysis = scan_folder(sub, out_dir)\n",
    "        if var_df is None:\n",
    "            master_rows.append({\"folder\": sub.name, \"status\": \"no_dta\"})\n",
    "            continue\n",
    "\n",
    "        analysis_name = analysis.name if analysis else \"\"\n",
    "        out_dta = None\n",
    "        if analysis:\n",
    "            out_dta = merge_raw_into_analysis(sub, analysis, var_df, out_dir)\n",
    "\n",
    "        master_rows.append({\n",
    "            \"folder\": sub.name,\n",
    "            \"scanned_files\": len(var_df[\"file\"].unique()),\n",
    "            \"raw_candidates\": 0 if cand_df is None else len(cand_df),\n",
    "            \"analysis_file\": analysis_name,\n",
    "            \"output_dta\": \"\" if out_dta is None else str(out_dta),\n",
    "            \"status\": \"ok\" if analysis else \"no_analysis_detected\"\n",
    "        })\n",
    "\n",
    "    pd.DataFrame(master_rows).to_csv(out_root / \"master_summary.csv\", index=False)\n",
    "    print(\"\\n[DONE] See:\", out_root)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00714811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Arora_et_al_2021_Data 스캔 중...\n",
      "▶ Azoulay_et_al_2019_Data 스캔 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\starw\\AppData\\Local\\Temp\\ipykernel_22628\\1423614591.py:48: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  df = pd.read_stata(f, convert_categoricals=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Berkouwer_and_Dean_2022_Data 스캔 중...\n",
      "▶ Cao_and_Chen_2022_Data 스캔 중...\n",
      "▶ Carranza_et_al_2022_Data 스캔 중...\n",
      "▶ Chen_and_Roth_2023_Data 스캔 중...\n",
      "▶ Fetzer_et_al_2021_Data 스캔 중...\n",
      "[WARN] C:\\Users\\starw\\OneDrive\\바탕 화면\\dta\\Fetzer_et_al_2021_Data\\REPOSITORY\\PROCESSING_FILES\\DATA_INPUTS\\DEPLOYMENT\\ISAFSTRENGTH.dta 읽기 실패: Version of given Stata file is 110. pandas supports importing versions 105, 108, 111 (Stata 7SE), 113 (Stata 8/9), 114 (Stata 10/11), 115 (Stata 12), 117 (Stata 13), 118 (Stata 14/15/16),and 119 (Stata 15/16, over 32,767 variables).\n",
      "[WARN] C:\\Users\\starw\\OneDrive\\바탕 화면\\dta\\Fetzer_et_al_2021_Data\\REPOSITORY\\PROCESSING_FILES\\DATA_INPUTS\\DISTANCE_MEASURES\\NEARROADS.dta 읽기 실패: Version of given Stata file is 110. pandas supports importing versions 105, 108, 111 (Stata 7SE), 113 (Stata 8/9), 114 (Stata 10/11), 115 (Stata 12), 117 (Stata 13), 118 (Stata 14/15/16),and 119 (Stata 15/16, over 32,767 variables).\n",
      "[WARN] C:\\Users\\starw\\OneDrive\\바탕 화면\\dta\\Fetzer_et_al_2021_Data\\REPOSITORY\\PROCESSING_FILES\\DATA_INPUTS\\DISTANCE_MEASURES\\TRAVELDISTNEARMILAIRPORT.dta 읽기 실패: Version of given Stata file is 110. pandas supports importing versions 105, 108, 111 (Stata 7SE), 113 (Stata 8/9), 114 (Stata 10/11), 115 (Stata 12), 117 (Stata 13), 118 (Stata 14/15/16),and 119 (Stata 15/16, over 32,767 variables).\n",
      "▶ Hjort_and_Poulsen_2019_Data 스캔 중...\n",
      "▶ Johnson_2020_Data 스캔 중...\n",
      "▶ Moretti_2021_Data 스캔 중...\n",
      "▶ Rogall_2021_Data 스캔 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\starw\\AppData\\Local\\Temp\\ipykernel_22628\\1423614591.py:48: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  df = pd.read_stata(f, convert_categoricals=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DONE] 최종 요약표 저장 완료 → C:\\Users\\starw\\OneDrive\\바탕 화면\\dta_output\\all_papers_variable_map.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "# ======== 사용자 경로 직접 지정 ========\n",
    "ROOT_DIR = Path(r\"C:\\Users\\starw\\OneDrive\\바탕 화면\\dta\")          # 11개 상위폴더가 있는 경로\n",
    "OUTPUT_DIR = Path(r\"C:\\Users\\starw\\OneDrive\\바탕 화면\\dta_output\")  # 결과 저장할 경로\n",
    "# ====================================\n",
    "\n",
    "# 키워드 그룹 정의\n",
    "KEYWORD_GROUPS = {\n",
    "    \"internet\": [\"internet\", \"fastinternet\", \"broadband\"],\n",
    "    \"earnings\": [\"earnings\", \"income\", \"wage\", \"salary\"],\n",
    "    \"hours\": [\"hours\", \"hrs\", \"time_worked\"],\n",
    "    \"conflict\": [\"incident\", \"conflict\", \"killings\"],\n",
    "    \"pubs\": [\"publication\", \"citations\", \"patent\"],\n",
    "    \"rebellion\": [\"rebellion\", \"rebel\"],\n",
    "    \"violations\": [\"violation\", \"penalty\", \"fine\"],\n",
    "    \"cluster\": [\"cluster_size\"],\n",
    "}\n",
    "\n",
    "# 추천 변환식\n",
    "SUGGESTED_TRANSFORMS = {\n",
    "    \"internet\": \"loginternet = log(1+internet) OR asinh(internet)\",\n",
    "    \"earnings\": \"asinh(earnings)\",\n",
    "    \"hours\": \"asinh(hours)\",\n",
    "    \"conflict\": \"log(1+incidents/killings)\",\n",
    "    \"pubs\": \"log(publications), log(citations)\",\n",
    "    \"rebellion\": \"asinh(rebellions per capita)\",\n",
    "    \"violations\": \"log(violations), log(penalties)\",\n",
    "    \"cluster\": \"log(cluster size)\",\n",
    "}\n",
    "\n",
    "def detect_group(varname: str):\n",
    "    v = varname.lower()\n",
    "    for g, kws in KEYWORD_GROUPS.items():\n",
    "        if any(kw in v for kw in kws):\n",
    "            return g\n",
    "    return None\n",
    "\n",
    "def scan_folder(folder: Path):\n",
    "    \"\"\"폴더 및 모든 하위폴더에서 .dta 파일 탐색\"\"\"\n",
    "    rows = []\n",
    "    for f in folder.rglob(\"*.dta\"):   # 하위폴더까지 전부 검색\n",
    "        try:\n",
    "            df = pd.read_stata(f, convert_categoricals=False)\n",
    "            for col in df.columns:\n",
    "                g = detect_group(col)\n",
    "                if g:\n",
    "                    rows.append({\n",
    "                        \"Paper\": folder.name,          # 최상위 폴더 이름 (논문 단위)\n",
    "                        \"Subfolder\": str(f.parent),   # 파일이 속한 하위폴더 경로\n",
    "                        \"File\": f.name,\n",
    "                        \"Original Variable(s)\": col,\n",
    "                        \"Transformed Variable(s)\": SUGGESTED_TRANSFORMS.get(g, \"\")\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] {f} 읽기 실패: {e}\")\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def main():\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    all_dfs = []\n",
    "\n",
    "    for folder in sorted(ROOT_DIR.iterdir()):\n",
    "        if folder.is_dir():\n",
    "            print(f\"▶ {folder.name} 스캔 중...\")\n",
    "            df = scan_folder(folder)\n",
    "            if not df.empty:\n",
    "                df.to_csv(OUTPUT_DIR / f\"{folder.name}_variable_map.csv\", index=False)\n",
    "                all_dfs.append(df)\n",
    "\n",
    "    if all_dfs:\n",
    "        final = pd.concat(all_dfs, ignore_index=True)\n",
    "        final.to_excel(OUTPUT_DIR / \"all_papers_variable_map.xlsx\", index=False)\n",
    "        print(f\"\\n[DONE] 최종 요약표 저장 완료 → {OUTPUT_DIR/'all_papers_variable_map.xlsx'}\")\n",
    "    else:\n",
    "        print(\"\\n[INFO] 원시 변수 후보를 찾지 못했습니다.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b07804c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyreadstat\n",
      "  Downloading pyreadstat-1.3.1.tar.gz (610 kB)\n",
      "     ---------------------------------------- 0.0/610.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 610.8/610.8 kB 7.4 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting narwhals>=2.0 (from pyreadstat)\n",
      "  Downloading narwhals-2.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\starw\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyreadstat) (1.24.3)\n",
      "Downloading narwhals-2.5.0-py3-none-any.whl (407 kB)\n",
      "Building wheels for collected packages: pyreadstat\n",
      "  Building wheel for pyreadstat (pyproject.toml): started\n",
      "  Building wheel for pyreadstat (pyproject.toml): finished with status 'error'\n",
      "Failed to build pyreadstat\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for pyreadstat (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [4 lines of output]\n",
      "      [1/3] Cythonizing pyreadstat/_readstat_parser.pyx\n",
      "      [2/3] Cythonizing pyreadstat/_readstat_writer.pyx\n",
      "      [3/3] Cythonizing pyreadstat/pyreadstat.pyx\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for pyreadstat\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (pyreadstat)\n"
     ]
    }
   ],
   "source": [
    "pip install pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dea11c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ 스캔 중: Arora_et_al_2021_Data\n",
      "▶ 스캔 중: Azoulay_et_al_2019_Data\n",
      "▶ 스캔 중: Berkouwer_and_Dean_2022_Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\starw\\AppData\\Local\\Temp\\ipykernel_22628\\2648116053.py:79: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  first = next(iter(reader))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ 스캔 중: Cao_and_Chen_2022_Data\n",
      "▶ 스캔 중: Carranza_et_al_2022_Data\n",
      "▶ 스캔 중: Chen_and_Roth_2023_Data\n",
      "▶ 스캔 중: Fetzer_et_al_2021_Data\n",
      "[WARN] 못 읽음: C:\\Users\\starw\\OneDrive\\바탕 화면\\dta\\Fetzer_et_al_2021_Data\\REPOSITORY\\PROCESSING_FILES\\DATA_INPUTS\\DEPLOYMENT\\ISAFSTRENGTH.dta\n",
      "[WARN] 못 읽음: C:\\Users\\starw\\OneDrive\\바탕 화면\\dta\\Fetzer_et_al_2021_Data\\REPOSITORY\\PROCESSING_FILES\\DATA_INPUTS\\DISTANCE_MEASURES\\NEARROADS.dta\n",
      "[WARN] 못 읽음: C:\\Users\\starw\\OneDrive\\바탕 화면\\dta\\Fetzer_et_al_2021_Data\\REPOSITORY\\PROCESSING_FILES\\DATA_INPUTS\\DISTANCE_MEASURES\\TRAVELDISTNEARMILAIRPORT.dta\n",
      "▶ 스캔 중: Hjort_and_Poulsen_2019_Data\n",
      "▶ 스캔 중: Johnson_2020_Data\n",
      "▶ 스캔 중: Moretti_2021_Data\n",
      "▶ 스캔 중: Rogall_2021_Data\n",
      "\n",
      "[DONE] 통합표 저장 완료 → C:\\Users\\starw\\OneDrive\\바탕 화면\\dta_output\\all_papers_variable_map.xlsx\n",
      "[INFO] 일부 파일은 다른 방식으로도 실패했습니다. 로그 확인: C:\\Users\\starw\\OneDrive\\바탕 화면\\dta_output\\read_errors.log\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings, traceback, datetime\n",
    "\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "# ======== 사용자 경로 ========\n",
    "ROOT_DIR = Path(r\"C:\\Users\\starw\\OneDrive\\바탕 화면\\dta\")          # 11개 상위폴더가 있는 경로(하위폴더들 포함)\n",
    "OUTPUT_DIR = Path(r\"C:\\Users\\starw\\OneDrive\\바탕 화면\\dta_output\")  # 결과 저장 경로\n",
    "# ===========================\n",
    "\n",
    "# pyreadstat 사용 가능 여부 체크\n",
    "try:\n",
    "    import pyreadstat\n",
    "    HAS_PYREADSTAT = True\n",
    "except Exception:\n",
    "    HAS_PYREADSTAT = False\n",
    "\n",
    "# 키워드 그룹(원시변수 탐지용)\n",
    "KEYWORD_GROUPS = {\n",
    "    \"internet\": [\"internet\", \"fastinternet\", \"broadband\", \"speed\", \"kbps\", \"mbps\", \"users\", \"usage\", \"penetration\"],\n",
    "    \"earnings\": [\"earnings\", \"income\", \"revenue\", \"wage\", \"wages\", \"salary\", \"pay\"],\n",
    "    \"hours\": [\"hours\", \"hrs\", \"time_worked\"],\n",
    "    \"conflict\": [\"incident\", \"incidents\", \"conflict\", \"sigacts\", \"killings\", \"victims\", \"ied\", \"direct_fire\", \"casualty\"],\n",
    "    \"pubs\": [\"publication\", \"publications\", \"paper\", \"papers\", \"patent\", \"patents\", \"citation\", \"citations\"],\n",
    "    \"rebellion\": [\"rebellion\", \"rebellions\", \"rebel\", \"uprising\"],\n",
    "    \"violations\": [\"violation\", \"violations\", \"penalty\", \"penalties\", \"fine\", \"fines\"],\n",
    "    \"cluster\": [\"cluster_size\", \"field_size\", \"peer_count\", \"inventors\"],\n",
    "}\n",
    "\n",
    "# 추천 변환식(보고용, 실제 계산 X)\n",
    "SUGGESTED_TRANSFORMS = {\n",
    "    \"internet\": \"loginternet = log(1+internet) OR asinh(internet)\",\n",
    "    \"earnings\": \"asinh(earnings) / asinh(wages)\",\n",
    "    \"hours\": \"asinh(hours)\",\n",
    "    \"conflict\": \"log(1+incidents/killings)\",\n",
    "    \"pubs\": \"log(publications) / log(citations)\",\n",
    "    \"rebellion\": \"asinh(rebellions per capita)\",\n",
    "    \"violations\": \"log(violations) / log(penalties)\",\n",
    "    \"cluster\": \"log(cluster size)\",\n",
    "}\n",
    "\n",
    "def log_error(log_path: Path, file_path: Path, err: Exception):\n",
    "    log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"[{datetime.datetime.now()}] {file_path}\\n\")\n",
    "        f.write(\"\".join(traceback.format_exception_only(type(err), err)))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "def detect_group(varname: str):\n",
    "    v = varname.lower()\n",
    "    for g, kws in KEYWORD_GROUPS.items():\n",
    "        if any(kw in v for kw in kws):\n",
    "            return g\n",
    "    return None\n",
    "\n",
    "def get_columns_from_dta(file_path: Path, error_log: Path):\n",
    "    \"\"\"\n",
    "    최대한 안전하게 .dta의 컬럼 목록을 얻는다.\n",
    "    1) pyreadstat (가능하면 row_limit=1) → meta.column_names 우선\n",
    "    2) pandas.read_stata(chunksize=1)\n",
    "    3) pandas.read_stata(그냥 전체) 마지막 시도\n",
    "    실패하면 None 반환하고 error_log에 기록\n",
    "    \"\"\"\n",
    "    # 1) pyreadstat (가장 호환성 넓음)\n",
    "    if HAS_PYREADSTAT:\n",
    "        try:\n",
    "            # 메모리 최소: 1행만\n",
    "            df, meta = pyreadstat.read_dta(str(file_path), row_limit=1, apply_value_formats=False)\n",
    "            cols = meta.column_names if meta and meta.column_names else list(df.columns)\n",
    "            return cols\n",
    "        except Exception as e:\n",
    "            log_error(error_log, file_path, e)\n",
    "            # 계속 진행 (다음 방법으로 재시도)\n",
    "\n",
    "    # 2) pandas: chunksize=1로 가볍게\n",
    "    try:\n",
    "        reader = pd.read_stata(file_path, convert_categoricals=False, chunksize=1)\n",
    "        first = next(iter(reader))\n",
    "        return list(first.columns)\n",
    "    except Exception as e:\n",
    "        log_error(error_log, file_path, e)\n",
    "\n",
    "    # 3) pandas: full read(마지막 시도 – 파일이 작을 때만 성공 가능)\n",
    "    try:\n",
    "        df = pd.read_stata(file_path, convert_categoricals=False)\n",
    "        return list(df.columns)\n",
    "    except Exception as e:\n",
    "        log_error(error_log, file_path, e)\n",
    "        return None\n",
    "\n",
    "def scan_one_top_folder(top_folder: Path, out_dir: Path, error_log: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    상위폴더(논문 1개 단위) 아래 모든 하위폴더를 rglob로 훑어 .dta 컬럼을 모아\n",
    "    Paper | Subfolder | File | Original Variable(s) | Transformed Variable(s)\n",
    "    형태로 DF 반환\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for f in top_folder.rglob(\"*.dta\"):\n",
    "        cols = get_columns_from_dta(f, error_log)\n",
    "        if not cols:\n",
    "            print(f\"[WARN] 못 읽음: {f}\")\n",
    "            continue\n",
    "        for col in cols:\n",
    "            g = detect_group(col)\n",
    "            if g:\n",
    "                rows.append({\n",
    "                    \"Paper\": top_folder.name,       # 최상위 폴더(논문)\n",
    "                    \"Subfolder\": str(f.parent),     # 실제 파일이 있는 경로\n",
    "                    \"File\": f.name,\n",
    "                    \"Original Variable(s)\": col,\n",
    "                    \"Transformed Variable(s)\": SUGGESTED_TRANSFORMS.get(g, \"\")\n",
    "                })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def main():\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    error_log = OUTPUT_DIR / \"read_errors.log\"\n",
    "    # 초기화\n",
    "    if error_log.exists():\n",
    "        error_log.unlink()\n",
    "\n",
    "    all_tables = []\n",
    "\n",
    "    for folder in sorted(ROOT_DIR.iterdir(), key=lambda p: p.name.lower()):\n",
    "        if not folder.is_dir():\n",
    "            continue\n",
    "        print(f\"▶ 스캔 중: {folder.name}\")\n",
    "        df = scan_one_top_folder(folder, OUTPUT_DIR, error_log)\n",
    "        if not df.empty:\n",
    "            df.to_csv(OUTPUT_DIR / f\"{folder.name}_variable_map.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "            all_tables.append(df)\n",
    "\n",
    "    if all_tables:\n",
    "        final = pd.concat(all_tables, ignore_index=True)\n",
    "        final.to_excel(OUTPUT_DIR / \"all_papers_variable_map.xlsx\", index=False)\n",
    "        print(f\"\\n[DONE] 통합표 저장 완료 → {OUTPUT_DIR/'all_papers_variable_map.xlsx'}\")\n",
    "        if error_log.exists():\n",
    "            print(f\"[INFO] 일부 파일은 다른 방식으로도 실패했습니다. 로그 확인: {error_log}\")\n",
    "    else:\n",
    "        print(\"\\n[INFO] 스캔 결과가 비었습니다. 경로/권한/파일 손상 여부를 확인하세요.\")\n",
    "        if error_log.exists():\n",
    "            print(f\"[INFO] 에러 로그: {error_log}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
