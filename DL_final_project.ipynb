{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "922da1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: 필요한 라이브러리 설치\n",
    "!pip install transformers peft datasets accelerate yfinance scikit-learn kagglehub -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5f5e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "import kagglehub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3222345b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_raw</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Currently, the company foresees its pre-tax pr...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The agreement strengthens our long-term partne...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Talvivaara Mining Company Plc Talvivaara Minin...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Snap Shares Tumble As Short Sellers Move In</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3 Steps to Creating the Company Culture You Want</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label_raw  label\n",
       "0  Currently, the company foresees its pre-tax pr...  negative      0\n",
       "1  The agreement strengthens our long-term partne...  positive      2\n",
       "2  Talvivaara Mining Company Plc Talvivaara Minin...  positive      2\n",
       "3        Snap Shares Tumble As Short Sellers Move In  negative      0\n",
       "4   3 Steps to Creating the Company Culture You Want  positive      2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_thor_2012 = r\"C:\\Users\\starw\\Downloads\\data_2012_frame_thor_output.json\"\n",
    "path_thor_business = r\"C:\\Users\\starw\\Downloads\\data_business_thor_output_1200.json\"\n",
    "\n",
    "df_2012 = pd.read_json(path_thor_2012)\n",
    "df_business = pd.read_json(path_thor_business)\n",
    "\n",
    "df_2012_sub = df_2012[[\"headline\", \"polarity\"]].copy()\n",
    "df_business_sub = df_business[[\"headline\", \"polarity\"]].copy()\n",
    "\n",
    "df_2012_sub.columns = [\"text\", \"label_raw\"]\n",
    "df_business_sub.columns = [\"text\", \"label_raw\"]\n",
    "\n",
    "df_thor = pd.concat([df_2012_sub, df_business_sub], ignore_index=True)\n",
    "df_thor[\"label_raw\"] = df_thor[\"label_raw\"].str.lower().str.strip()\n",
    "label_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "df_thor[\"label\"] = df_thor[\"label_raw\"].map(label_map)\n",
    "\n",
    "df_thor = df_thor.dropna(subset=[\"label\"])\n",
    "df_thor.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98b93828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPB path: C:\\Users\\starw\\.cache\\kagglehub\\datasets\\ankurzing\\sentiment-analysis-for-financial-news\\versions\\5\n",
      "FPB size: 4846\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_raw</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label_raw                                               text  label\n",
       "0   neutral  According to Gran , the company has no plans t...      1\n",
       "1   neutral  Technopolis plans to develop in stages an area...      1\n",
       "2  negative  The international electronic industry company ...      0\n",
       "3  positive  With the new production plant the company woul...      2\n",
       "4  positive  According to the company 's updated strategy f...      2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FinancialPhraseBank 다운로드\n",
    "path_fpb = kagglehub.dataset_download(\"ankurzing/sentiment-analysis-for-financial-news\")\n",
    "print(\"FPB path:\", path_fpb)\n",
    "\n",
    "csv_path = list(Path(path_fpb).rglob(\"*.csv\"))[0]\n",
    "fpb = pd.read_csv(csv_path, names=[\"label_raw\", \"text\"], encoding=\"latin-1\")\n",
    "fpb[\"label_raw\"] = fpb[\"label_raw\"].str.lower().str.strip()\n",
    "\n",
    "fpb_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "fpb[\"label\"] = fpb[\"label_raw\"].map(fpb_map)\n",
    "fpb = fpb.dropna(subset=[\"label\"])\n",
    "\n",
    "print(\"FPB size:\", len(fpb))\n",
    "fpb.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8e6be49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US News path: C:\\Users\\starw\\.cache\\kagglehub\\datasets\\jeet2016\\us-financial-news-articles\\versions\\1\n",
      "Total JSON files: 306242\n"
     ]
    }
   ],
   "source": [
    "# 대용량 뉴스 데이터 다운로드\n",
    "path_usnews = kagglehub.dataset_download(\"jeet2016/us-financial-news-articles\")\n",
    "print(\"US News path:\", path_usnews)\n",
    "\n",
    "root_us = Path(path_usnews)\n",
    "json_files = list(root_us.rglob(\"*.json\"))\n",
    "print(\"Total JSON files:\", len(json_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adcee026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed US news: (60000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>Emerging markets are set for an even bigger ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>Cramer reflects on how Trump's actions are fue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>The Wall Street Journal: Peter Thiel’s VC firm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>Hoda Kotb Will Replace Matt Lauer on NBC’s ‘To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>UK's Compass says new CEO to start Jan 1 after...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                              title\n",
       "0  2018-01-03  Emerging markets are set for an even bigger ra...\n",
       "1  2018-01-03  Cramer reflects on how Trump's actions are fue...\n",
       "2  2018-01-03  The Wall Street Journal: Peter Thiel’s VC firm...\n",
       "3  2018-01-02  Hoda Kotb Will Replace Matt Lauer on NBC’s ‘To...\n",
       "4  2018-01-01  UK's Compass says new CEO to start Jan 1 after..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = []\n",
    "dates = []\n",
    "\n",
    "MAX_FILES = 60000  # 속도: 필요하면 줄여도 됨\n",
    "\n",
    "for i, fp in enumerate(json_files):\n",
    "    if i >= MAX_FILES:\n",
    "        break\n",
    "    try:\n",
    "        data = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # title 찾기\n",
    "    title = data.get(\"title\") or data.get(\"thread\", {}).get(\"title\")\n",
    "    if not title:\n",
    "        continue\n",
    "    \n",
    "    # published date 찾기\n",
    "    published = data.get(\"published\") or data.get(\"thread\", {}).get(\"published\")\n",
    "    if not published:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        dt = pd.to_datetime(published).date()\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    titles.append(title)\n",
    "    dates.append(dt)\n",
    "\n",
    "news_df = pd.DataFrame({\"date\": dates, \"title\": titles})\n",
    "print(\"Parsed US news:\", news_df.shape)\n",
    "news_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b3875c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule-based labeled news: 102\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>China's 2017 exports rose 10.8% in yuan terms ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>BlackRock earnings beat expectations; assets u...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>Intel just warned that its patches can cause p...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>2018-01-16</td>\n",
       "      <td>Jared Kushner reportedly was warned about his ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>2017 Chinese foreign direct investment fell 35...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date                                               text  label\n",
       "260   2018-01-12  China's 2017 exports rose 10.8% in yuan terms ...    2.0\n",
       "560   2018-01-12  BlackRock earnings beat expectations; assets u...    2.0\n",
       "572   2018-01-12  Intel just warned that its patches can cause p...    0.0\n",
       "811   2018-01-16  Jared Kushner reportedly was warned about his ...    0.0\n",
       "1075  2018-01-17  2017 Chinese foreign direct investment fell 35...    0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_keywords = [\"soared\", \"jumped\", \"skyrocketed\", \"surged\", \"climbed\", \"beat expectations\"]\n",
    "negative_keywords = [\"slumped\", \"tumbled\", \"plunged\", \"collapsed\", \"fell\", \"warned\"]\n",
    "\n",
    "def rule_label(t):\n",
    "    t = t.lower()\n",
    "    if any(w in t for w in positive_keywords):\n",
    "        return 2\n",
    "    if any(w in t for w in negative_keywords):\n",
    "        return 0\n",
    "    return None\n",
    "\n",
    "news_df[\"label\"] = news_df[\"title\"].apply(rule_label)\n",
    "news_labeled = news_df.dropna(subset=[\"label\"]).copy()\n",
    "news_labeled.rename(columns={\"title\": \"text\"}, inplace=True)\n",
    "\n",
    "print(\"Rule-based labeled news:\", len(news_labeled))\n",
    "news_labeled.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ede6e553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 6151\n",
      "label\n",
      "1.0    3061\n",
      "2.0    1821\n",
      "0.0    1269\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  According to Gran , the company has no plans t...    1.0\n",
       "1  Technopolis plans to develop in stages an area...    1.0\n",
       "2  The international electronic industry company ...    0.0\n",
       "3  With the new production plant the company woul...    2.0\n",
       "4  According to the company 's updated strategy f...    2.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = pd.concat([\n",
    "    fpb[[\"text\", \"label\"]],\n",
    "    df_thor[[\"text\", \"label\"]],\n",
    "    news_labeled[[\"text\", \"label\"]],\n",
    "], ignore_index=True)\n",
    "\n",
    "full_data = full_data.dropna(subset=[\"text\", \"label\"]).reset_index(drop=True)\n",
    "print(\"Total dataset size:\", len(full_data))\n",
    "print(full_data[\"label\"].value_counts())\n",
    "full_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "004675d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5535, 2), (616, 2))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    full_data,\n",
    "    test_size=0.1,\n",
    "    stratify=full_data[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_df.shape, val_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd7c2a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 297,219 || all params: 109,781,766 || trainable%: 0.2707\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x000001E7F56CEDE0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\starw\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\starw\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"ProsusAI/finbert\",\n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "# Set problem type explicitly\n",
    "base_model.config.problem_type = \"single_label_classification\"\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Ensure problem type is preserved after PEFT\n",
    "model.config.problem_type = \"single_label_classification\"\n",
    "\n",
    "print(model.print_trainable_parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cc73ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.labels = df[\"label\"].astype(int).tolist()\n",
    "        self.tk = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tk(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_ds = FinDataset(train_df, tokenizer)\n",
    "val_ds = FinDataset(val_df, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15307be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"macro\")\n",
    "    }\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./finbert_lora_all\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=200,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f944fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\starw\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1038' max='1038' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1038/1038 2:39:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.150900</td>\n",
       "      <td>0.747311</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.589293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>0.539554</td>\n",
       "      <td>0.792208</td>\n",
       "      <td>0.771372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.545000</td>\n",
       "      <td>0.515551</td>\n",
       "      <td>0.801948</td>\n",
       "      <td>0.785491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\starw\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\starw\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\starw\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='393' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 49:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5155511498451233,\n",
       " 'eval_accuracy': 0.801948051948052,\n",
       " 'eval_f1': 0.7854905367374053,\n",
       " 'eval_runtime': 1122.8734,\n",
       " 'eval_samples_per_second': 0.549,\n",
       " 'eval_steps_per_second': 0.018,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f2b2a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter financial dataset path: C:\\Users\\starw\\.cache\\kagglehub\\datasets\\borhanitrash\\twitter-financial-news-sentiment-dataset\\versions\\1\n",
      "Files: ['README.md', 'sent_dataset_meta.txt', 'sent_train.csv', 'sent_valid.csv']\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Twitter Financial News Sentiment Dataset 다운로드\n",
    "\n",
    "import os\n",
    "import kagglehub\n",
    "\n",
    "# Kaggle에서 데이터 내려받기\n",
    "twitter_path = kagglehub.dataset_download(\n",
    "    \"borhanitrash/twitter-financial-news-sentiment-dataset\"\n",
    ")\n",
    "\n",
    "print(\"Twitter financial dataset path:\", twitter_path)\n",
    "print(\"Files:\", os.listdir(twitter_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2326f784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (9543, 2)\n",
      "Valid shape: (2388, 2)\n",
      "Train columns: ['text', 'label']\n",
      "Valid columns: ['text', 'label']\n",
      "Detected text/label columns (train): text label\n",
      "Detected text/label columns (valid): text label\n",
      "                                                text  label_raw\n",
      "0  $BYND - JPMorgan reels in expectations on Beyo...          0\n",
      "1  $CCL $RCL - Nomura points to bookings weakness...          0\n",
      "2  $CX - Cemex cut at Credit Suisse, J.P. Morgan ...          0\n",
      "3  $ESS: BTIG Research cuts to Neutral https://t....          0\n",
      "4  $FNKO - Funko slides after Piper Jaffray PT cu...          0\n",
      "                                                text  label_raw\n",
      "0  $ALLY - Ally Financial pulls outlook https://t...          0\n",
      "1  $DELL $HPE - Dell, HPE targets trimmed on comp...          0\n",
      "2  $PRTY - Moody's turns negative on Party City h...          0\n",
      "3                   $SAN: Deutsche Bank cuts to Hold          0\n",
      "4                  $SITC: Compass Point cuts to Sell          0\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: CSV 로딩 및 텍스트/라벨 컬럼 식별\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 보통 파일 이름이 이런 식이라서 우선 이렇게 시도\n",
    "train_csv_path = os.path.join(twitter_path, \"sent_train.csv\")\n",
    "valid_csv_path = os.path.join(twitter_path, \"sent_valid.csv\")\n",
    "\n",
    "df_train_tw = pd.read_csv(train_csv_path)\n",
    "df_valid_tw = pd.read_csv(valid_csv_path)\n",
    "\n",
    "print(\"Train shape:\", df_train_tw.shape)\n",
    "print(\"Valid shape:\", df_valid_tw.shape)\n",
    "print(\"Train columns:\", df_train_tw.columns.tolist())\n",
    "print(\"Valid columns:\", df_valid_tw.columns.tolist())\n",
    "\n",
    "# 텍스트/라벨 컬럼 자동 탐색 (데이터셋 구조가 조금 달라도 버티게)\n",
    "def detect_text_label_cols(df):\n",
    "    text_candidates = [\"text\", \"sentence\", \"tweet\", \"content\"]\n",
    "    label_candidates = [\"label\", \"sentiment\", \"target\", \"labels\"]\n",
    "\n",
    "    text_col = None\n",
    "    label_col = None\n",
    "\n",
    "    for c in df.columns:\n",
    "        if c.lower() in text_candidates:\n",
    "            text_col = c\n",
    "        if c.lower() in label_candidates:\n",
    "            label_col = c\n",
    "\n",
    "    if text_col is None or label_col is None:\n",
    "        raise ValueError(\n",
    "            f\"텍스트/라벨 컬럼을 찾지 못했습니다. 실제 컬럼명을 확인해서 코드에서 직접 지정해 주세요. \"\n",
    "            f\"(현재 컬럼들: {df.columns.tolist()})\"\n",
    "        )\n",
    "    return text_col, label_col\n",
    "\n",
    "text_col_train, label_col_train = detect_text_label_cols(df_train_tw)\n",
    "text_col_valid, label_col_valid = detect_text_label_cols(df_valid_tw)\n",
    "\n",
    "print(\"Detected text/label columns (train):\", text_col_train, label_col_train)\n",
    "print(\"Detected text/label columns (valid):\", text_col_valid, label_col_valid)\n",
    "\n",
    "# 공통 포맷으로 정리\n",
    "df_train_tw = df_train_tw[[text_col_train, label_col_train]].rename(\n",
    "    columns={text_col_train: \"text\", label_col_train: \"label_raw\"}\n",
    ")\n",
    "df_valid_tw = df_valid_tw[[text_col_valid, label_col_valid]].rename(\n",
    "    columns={text_col_valid: \"text\", label_col_valid: \"label_raw\"}\n",
    ")\n",
    "\n",
    "print(df_train_tw.head())\n",
    "print(df_valid_tw.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b63295c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    6178\n",
      "2    1923\n",
      "0    1442\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    1566\n",
      "2     475\n",
      "0     347\n",
      "Name: count, dtype: int64\n",
      "Train unique texts: 9543 / 9543\n",
      "Valid unique texts: 2388 / 2388\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_raw</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5470</th>\n",
       "      <td>When staffers in WeWork's New York City headqu...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8982</th>\n",
       "      <td>ADMA Biologics down 9% premarket after pricing...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4704</th>\n",
       "      <td>Illegal Tender podcast: What it was like to be...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>The Splunk Data-to-Everything Platform Brings ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3146</th>\n",
       "      <td>OPEC's share of Indian oil imports in October ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label_raw  label\n",
       "5470  When staffers in WeWork's New York City headqu...          2      1\n",
       "8982  ADMA Biologics down 9% premarket after pricing...          0      0\n",
       "4704  Illegal Tender podcast: What it was like to be...          2      1\n",
       "1954  The Splunk Data-to-Everything Platform Brings ...          2      1\n",
       "3146  OPEC's share of Indian oil imports in October ...          2      1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 16: Twitter 라벨 → FinBERT 라벨로 매핑\n",
    "\n",
    "def map_twitter_label(v):\n",
    "    # 문자열일 때\n",
    "    if isinstance(v, str):\n",
    "        v_clean = v.strip().lower()\n",
    "        mapping_str = {\n",
    "            \"bearish\": 0,   # negative\n",
    "            \"neutral\": 1,\n",
    "            \"bullish\": 2,   # positive\n",
    "        }\n",
    "        if v_clean not in mapping_str:\n",
    "            raise ValueError(f\"예상치 못한 문자열 라벨: {v}\")\n",
    "        return mapping_str[v_clean]\n",
    "\n",
    "    # 숫자일 때 (HF 정의: 0=Bearish, 1=Bullish, 2=Neutral)\n",
    "    try:\n",
    "        v_int = int(v)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"라벨을 int로 변환할 수 없음: {v}\") from e\n",
    "\n",
    "    mapping_num = {0: 0, 2: 1, 1: 2}  # 0=neg,2=neu,1=pos로 재정렬\n",
    "    if v_int not in mapping_num:\n",
    "        raise ValueError(f\"예상치 못한 숫자 라벨: {v_int}\")\n",
    "    return mapping_num[v_int]\n",
    "\n",
    "df_train_tw[\"label\"] = df_train_tw[\"label_raw\"].apply(map_twitter_label)\n",
    "df_valid_tw[\"label\"] = df_valid_tw[\"label_raw\"].apply(map_twitter_label)\n",
    "\n",
    "print(df_train_tw[\"label\"].value_counts())\n",
    "print(df_valid_tw[\"label\"].value_counts())\n",
    "\n",
    "# 중복 제거 및 기본 확인\n",
    "print(\"Train unique texts:\", df_train_tw[\"text\"].nunique(), \"/\", len(df_train_tw))\n",
    "print(\"Valid unique texts:\", df_valid_tw[\"text\"].nunique(), \"/\", len(df_valid_tw))\n",
    "\n",
    "df_train_tw.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f21252be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16.5: tokenizer 다시 로드 (Cell 17 실행 전에!)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 파인튜닝에 사용한 같은 체크포인트로 맞춰줄 것\n",
    "model_name_or_path = \"ProsusAI/finbert\"      # 또는 너의 fine-tuned 모델 디렉토리 경로\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa886aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96496a52e1234e4a8ee4527522cd50aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11931 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 11931\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 17: HuggingFace Dataset으로 변환 후 토크나이즈\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# 평가용으로 validation split만 사용할 수도 있지만,\n",
    "# 여기서는 train+valid 모두 합쳐서 \"외부 평가셋\"으로 사용 (원하는 대로 조절 가능)\n",
    "df_twitter_all = pd.concat([df_train_tw[[\"text\", \"label\"]],\n",
    "                            df_valid_tw[[\"text\", \"label\"]]],\n",
    "                           ignore_index=True)\n",
    "\n",
    "twitter_dataset = Dataset.from_pandas(df_twitter_all)\n",
    "\n",
    "def preprocess_twitter(examples):\n",
    "    # 앞에서 쓰던 tokenize_function이 있으면 그대로 써도 됨\n",
    "    # 예: return tokenize_function(examples)\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "    tokenized[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized\n",
    "\n",
    "twitter_encoded = twitter_dataset.map(preprocess_twitter, batched=True)\n",
    "\n",
    "# Trainer용 포맷 설정\n",
    "twitter_encoded.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "twitter_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad32379e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\starw\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Twitter Financial News Sentiment – External Eval =====\n",
      "eval_loss: 0.6721\n",
      "eval_accuracy: 0.7261\n",
      "eval_f1: 0.6681\n",
      "eval_runtime: 1873.6256\n",
      "eval_samples_per_second: 6.3680\n",
      "eval_steps_per_second: 0.1990\n",
      "epoch: 3.0000\n"
     ]
    }
   ],
   "source": [
    "# Cell 18: Twitter 금융 트윗 감성 분류 성능 평가\n",
    "\n",
    "twitter_metrics = trainer.evaluate(twitter_encoded)\n",
    "print(\"===== Twitter Financial News Sentiment – External Eval =====\")\n",
    "for k, v in twitter_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\" if isinstance(v, (int, float)) else f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b3fb458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News data shape: (60000, 3)\n",
      "Date range: 2017-12-07 to 2018-02-28\n",
      "Predicting sentiment for news articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [10:35:54<00:00,  1.57it/s]       \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved sentiment_daily_summary.csv with 75 days\n",
      "    date_only  news_count  sent_mean  sent_std  sent_max  sent_min  pos_mean  \\\n",
      "0  2017-12-07           3  -0.333333   0.57735         0        -1  0.119003   \n",
      "1  2017-12-08           3   0.333333   0.57735         1         0  0.415521   \n",
      "2  2017-12-10           1   0.000000   0.00000         0         0  0.134113   \n",
      "3  2017-12-13           1   0.000000   0.00000         0         0  0.087634   \n",
      "4  2017-12-14           3   0.000000   0.00000         0         0  0.159874   \n",
      "5  2017-12-15           3   0.000000   1.00000         1        -1  0.256659   \n",
      "6  2017-12-18           1  -1.000000   0.00000        -1        -1  0.020795   \n",
      "7  2017-12-20           1   0.000000   0.00000         0         0  0.169697   \n",
      "8  2017-12-21           4   0.000000   0.00000         0         0  0.143694   \n",
      "9  2017-12-22           2   0.000000   0.00000         0         0  0.129280   \n",
      "\n",
      "   neg_mean  \n",
      "0  0.353256  \n",
      "1  0.079328  \n",
      "2  0.211022  \n",
      "3  0.090975  \n",
      "4  0.101922  \n",
      "5  0.422308  \n",
      "6  0.606820  \n",
      "7  0.082692  \n",
      "8  0.096611  \n",
      "9  0.093171  \n"
     ]
    }
   ],
   "source": [
    "# Cell 18.5: 뉴스 데이터에 감성 점수 예측하여 daily summary 생성\n",
    "\n",
    "# 1) 모델과 토크나이저 준비 (이미 로드되어 있으면 재사용)\n",
    "try:\n",
    "    model\n",
    "    tokenizer\n",
    "except NameError:\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"ProsusAI/finbert\",\n",
    "        num_labels=3\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base_model, \"./finbert_lora_all\")\n",
    "    model.config.problem_type = \"single_label_classification\"\n",
    "\n",
    "# 2) news_df에 날짜가 있어야 함 (Cell 6에서 생성)\n",
    "# news_df = DataFrame with columns: ['date', 'title']\n",
    "print(\"News data shape:\", news_df.shape)\n",
    "print(\"Date range:\", news_df['date'].min(), \"to\", news_df['date'].max())\n",
    "\n",
    "# 3) 각 뉴스에 대해 감성 점수 예측\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    \"\"\"텍스트에 대한 감성 점수 예측 (0=neg, 1=neu, 2=pos)\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)\n",
    "        pred_label = torch.argmax(probs, dim=-1).item()\n",
    "        \n",
    "    # 감성 점수: negative=-1, neutral=0, positive=1\n",
    "    sentiment_map = {0: -1, 1: 0, 2: 1}\n",
    "    return sentiment_map[pred_label], probs[0].cpu().numpy()\n",
    "\n",
    "# 4) 배치 처리로 예측 (속도 향상)\n",
    "sentiments = []\n",
    "pos_probs = []\n",
    "neg_probs = []\n",
    "\n",
    "print(\"Predicting sentiment for news articles...\")\n",
    "for idx, row in tqdm(news_df.iterrows(), total=len(news_df)):\n",
    "    try:\n",
    "        sent_score, probs = predict_sentiment(row['title'])\n",
    "        sentiments.append(sent_score)\n",
    "        pos_probs.append(probs[2])  # positive probability\n",
    "        neg_probs.append(probs[0])  # negative probability\n",
    "    except Exception as e:\n",
    "        sentiments.append(0)\n",
    "        pos_probs.append(0.33)\n",
    "        neg_probs.append(0.33)\n",
    "\n",
    "news_df['sentiment'] = sentiments\n",
    "news_df['pos_prob'] = pos_probs\n",
    "news_df['neg_prob'] = neg_probs\n",
    "\n",
    "# 5) 날짜별로 집계\n",
    "daily_sentiment = news_df.groupby('date').agg(\n",
    "    news_count=('sentiment', 'count'),\n",
    "    sent_mean=('sentiment', 'mean'),\n",
    "    sent_std=('sentiment', 'std'),\n",
    "    sent_max=('sentiment', 'max'),\n",
    "    sent_min=('sentiment', 'min'),\n",
    "    pos_mean=('pos_prob', 'mean'),\n",
    "    neg_mean=('neg_prob', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "daily_sentiment.columns = ['date_only', 'news_count', 'sent_mean', 'sent_std', \n",
    "                           'sent_max', 'sent_min', 'pos_mean', 'neg_mean']\n",
    "\n",
    "# std가 NaN인 경우 (뉴스 1개만 있는 날) 0으로 채우기\n",
    "daily_sentiment['sent_std'] = daily_sentiment['sent_std'].fillna(0)\n",
    "\n",
    "# 6) CSV로 저장\n",
    "daily_sentiment.to_csv(\"sentiment_daily_summary.csv\", index=False)\n",
    "print(f\"\\nSaved sentiment_daily_summary.csv with {len(daily_sentiment)} days\")\n",
    "print(daily_sentiment.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2e2de8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\starw\\AppData\\Local\\Temp\\ipykernel_16392\\1077789927.py:11: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  sp500 = yf.download(\"^GSPC\", start=\"2012-01-01\", end=\"2025-01-01\")\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (53, 11)\n",
      "   date_only  news_count  sent_mean  sent_std  sent_max  sent_min  pos_mean  \\\n",
      "0 2017-12-07           3  -0.333333   0.57735         0        -1  0.119003   \n",
      "1 2017-12-08           3   0.333333   0.57735         1         0  0.415521   \n",
      "2 2017-12-13           1   0.000000   0.00000         0         0  0.087634   \n",
      "3 2017-12-14           3   0.000000   0.00000         0         0  0.159874   \n",
      "4 2017-12-15           3   0.000000   1.00000         1        -1  0.256659   \n",
      "\n",
      "   neg_mean        Close       ret  ret_next  \n",
      "0  0.353256  2636.979980  0.002932  0.005506  \n",
      "1  0.079328  2651.500000  0.005506  0.003202  \n",
      "2  0.090975  2662.850098 -0.000473 -0.004071  \n",
      "3  0.101922  2652.010010 -0.004071  0.008974  \n",
      "4  0.422308  2675.810059  0.008974  0.005363  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# 1) sentiment summary 파일 불러오기\n",
    "sent_df = pd.read_csv(\"sentiment_daily_summary.csv\")  \n",
    "# 예: columns = ['date_only','news_count','sent_mean','sent_std','sent_max','sent_min','pos_mean','neg_mean']\n",
    "\n",
    "sent_df['date_only'] = pd.to_datetime(sent_df['date_only'])\n",
    "\n",
    "# 2) S&P500 가격 불러오기\n",
    "sp500 = yf.download(\"^GSPC\", start=\"2012-01-01\", end=\"2025-01-01\")\n",
    "\n",
    "# MultiIndex 컬럼을 flatten (yfinance가 MultiIndex를 반환하는 경우 대비)\n",
    "if isinstance(sp500.columns, pd.MultiIndex):\n",
    "    sp500.columns = sp500.columns.get_level_values(0)\n",
    "\n",
    "sp500 = sp500.reset_index()\n",
    "sp500 = sp500.rename(columns={\"Date\": \"date_only\"})\n",
    "\n",
    "# daily return 생성\n",
    "sp500[\"ret\"] = sp500[\"Close\"].pct_change()\n",
    "sp500[\"ret_next\"] = sp500[\"ret\"].shift(-1)\n",
    "\n",
    "# 3) sentiment + market merge\n",
    "merged = pd.merge(sent_df, sp500[[\"date_only\",\"Close\",\"ret\",\"ret_next\"]],\n",
    "                  on=\"date_only\", how=\"inner\")\n",
    "\n",
    "print(\"Merged shape:\", merged.shape)\n",
    "print(merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c181c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 42 Test size: 11\n",
      "Feature sets: baseline = 1 , sentiment = 7 , full = 8\n"
     ]
    }
   ],
   "source": [
    "# Cell 19: 특징/타깃 정의 (baseline vs sentiment 비교)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1) 감성 feature들\n",
    "sent_cols = [\n",
    "    \"news_count\",\n",
    "    \"sent_mean\",\n",
    "    \"sent_std\",\n",
    "    \"sent_max\",\n",
    "    \"sent_min\",\n",
    "    \"pos_mean\",\n",
    "    \"neg_mean\",\n",
    "]\n",
    "\n",
    "# 2) baseline feature (예: 전일 수익률 하나만)\n",
    "base_cols = [\"ret\"]\n",
    "\n",
    "# 3) full model: baseline + sentiment\n",
    "full_cols = base_cols + sent_cols\n",
    "\n",
    "# 결측치 정리\n",
    "data = merged.copy().dropna(subset=[\"ret_next\"])  # 라벨 없는 날 제거\n",
    "data[sent_cols] = data[sent_cols].fillna(0.0)\n",
    "\n",
    "# time-series니까 섞지 말고 뒤쪽 20%를 test로 사용\n",
    "split_idx = int(len(data) * 0.8)\n",
    "train_df = data.iloc[:split_idx].copy()\n",
    "test_df  = data.iloc[split_idx:].copy()\n",
    "\n",
    "y_train = train_df[\"ret_next\"].values\n",
    "y_test  = test_df[\"ret_next\"].values\n",
    "\n",
    "X_base_train = train_df[base_cols].values\n",
    "X_base_test  = test_df[base_cols].values\n",
    "\n",
    "X_sent_train = train_df[sent_cols].values\n",
    "X_sent_test  = test_df[sent_cols].values\n",
    "\n",
    "X_full_train = train_df[full_cols].values\n",
    "X_full_test  = test_df[full_cols].values\n",
    "\n",
    "print(\"Train size:\", len(train_df), \"Test size:\", len(test_df))\n",
    "print(\"Feature sets:\",\n",
    "      \"baseline =\", X_base_train.shape[1],\n",
    "      \", sentiment =\", X_sent_train.shape[1],\n",
    "      \", full =\", X_full_train.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eae34ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: RandomForest로 baseline / sentiment-only / full 모델 학습 및 예측\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_base = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "rf_sent = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "rf_full = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "rf_base.fit(X_base_train, y_train)\n",
    "rf_sent.fit(X_sent_train, y_train)\n",
    "rf_full.fit(X_full_train, y_train)\n",
    "\n",
    "y_pred_base = rf_base.predict(X_base_test)\n",
    "y_pred_sent = rf_sent.predict(X_sent_test)\n",
    "y_pred_full = rf_full.predict(X_full_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d6dc88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Baseline (ret only) =====\n",
      "R^2   : 0.2403\n",
      "RMSE  : 0.009271\n",
      "DirAcc: 0.7273\n",
      "===== Sentiment-only =====\n",
      "R^2   : -0.1175\n",
      "RMSE  : 0.011244\n",
      "DirAcc: 0.4545\n",
      "===== Full (ret + sentiment) =====\n",
      "R^2   : 0.1198\n",
      "RMSE  : 0.009979\n",
      "DirAcc: 0.6364\n"
     ]
    }
   ],
   "source": [
    "# Cell 21: 세 모델 성능 비교 (R², RMSE, 방향(up/down) 정확도)\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def eval_reg(y_true, y_pred, name=\"model\"):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    dir_true = (y_true > 0).astype(int)\n",
    "    dir_pred = (y_pred > 0).astype(int)\n",
    "    acc = (dir_true == dir_pred).mean()\n",
    "    print(f\"===== {name} =====\")\n",
    "    print(f\"R^2   : {r2:.4f}\")\n",
    "    print(f\"RMSE  : {rmse:.6f}\")\n",
    "    print(f\"DirAcc: {acc:.4f}\")\n",
    "    return r2, rmse, acc\n",
    "\n",
    "r2_b, rmse_b, acc_b = eval_reg(y_test, y_pred_base, \"Baseline (ret only)\")\n",
    "r2_s, rmse_s, acc_s = eval_reg(y_test, y_pred_sent, \"Sentiment-only\")\n",
    "r2_f, rmse_f, acc_f = eval_reg(y_test, y_pred_full, \"Full (ret + sentiment)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
